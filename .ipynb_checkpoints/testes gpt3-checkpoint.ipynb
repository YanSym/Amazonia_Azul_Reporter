{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import wget\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Os lixões e a omissão, um grave problema ambiental - Mar Sem Fim InícioOpiniãoExpediçõesCosta BrasileiraBrazilian Coast – English VersionPrimeira Viagem à AntárticaViagem do NaufrágioResgate do Mar Sem FimUnidades de ConservaçãoDocumentáriosCosta BrasileiraAntárticaUnidades de ConservaçãoO resgate do Mar Sem FimOceanosSaúde dos OceanosEmbarcaçõesAcidentesEmbarcações TípicasDe PesquisaNaufrágiosNavegadoresNaviosVeleirosVida MarinhaAmeaçados de ExtinçãoAvesCrustáceosCuriosasOutras EspéciesEspécies InvasorasMamíferos MarinhosPeixesTartarugasRegiões PolaresEcossistemasPoluiçãoÁreas ProtegidasIlhasLitoralPescaArtesanalPesca EsportivaPredatóriaMundo SubmarinoEsportesHistória MarítimaEnergiaExtração MineralVariadosPodcastAmazôniaClimaONGsTecnologiaSustentabilidadeSaiu na imprensaContatoAnuncieBiografiaBuscarMENUAprendaAtive-sePodcastInícioOceanosPoluiçãoOs lixões e a omissão, um grave problema ambientalOceanosPoluiçãoOs lixões e a omissão, um grave problema ambientalPor João Lara Mesquita - 17 de dezembro de 20211356 views Os lixões e a omissão, um grave problema ambientalO Brasil é um país pra lá de curioso. Dá um passo pra frente, um pro lado, outro pra trás, para só depois prosseguir no avanço, sempre a passos de cágado. Aqui há outra curiosidade típica leis que pegam, e leis que não pegam. Não conheço outro lugar onde este anacronismo ainda acontece. E assim vai se cristalizando o País do futuro. A gente nasce, cresce, envelhece, ouvindo a ladainha o País do futuro. Sei… De vez em quando ainda aparece nova autoridade embalada em fracassos seguidos que insiste o Brasil vai surpreender o mundo. Ora, já não surpreendemos o suficiente? O assunto de hoje são os lixões e a omissão, um grave problema ambiental.Eles fazem parte de nossa o os Barracões de zinco pendurados no morro, me pedindo socorro. Duas vergonhas nacionais Lixão da Estrutural, Distrito Federal, fechado em 2018. Imagem, Edilson RodriguesAgência Senado.Lixões e a omissãoA Lei n.º 12.3052010, que instituiu a Política Nacional de Resíduos Sólidos, parece que é uma destas que não pegou. Há mais de dez anos ela definiu que os lixões deveriam ter pletamente extintos até 2014.Leia tambémIndústria de cruzeiros deve ser sujeita a controleLixo nos rios de Ubatuba; estância balneária até quando?Mutirão de limpeza revela esculhambação em Fernando de NoronhaSete anos depois, mais da metade dos mais de 5.500 municípios brasileiros (51,5%) ainda descarta lixo de forma inadequada, sem qualquer tipo de tratamento. O pior é que o assunto ove a opinião pública. Parece até que estamos falando dos oceanos… Só isso justifica tanto atraso.Os dados são do relatório Panorama dos Resíduos Sólidos no Brasil, 2021, da Associação Brasileira das Empresas de Limpeza Pública e Resíduos Especiais (Abrelpe).Ainda em 2018 publicamos um post a respeito, onde dizíamos que o problema persiste passados nove anos da publicação da Lei. Todos os lixõesdeveriam ser fechados até 2014. Isso não aconteceu.PUBLICIDADE A maioria das cidades ainda mantém depósitos de lixo sem qualquer tratamento. Se continuarmos nesta balada, só cumpriremos o objetivo de reduzir os impactos do lixo em 2060. Mas o prazo estabelecido pela Organização das Nações Unidas (ONU) vai só até 2030. Hora de rever a Lei?Carlos Silva Filho, presidente da Abrelpe, disse ao jornal Valor O segmento (de tratamento do lixo urbano) avança a passos muito lentos, a despeito de todo o arcabouço legal paraos lixões no Brasil.O lixo por região do BrasilO relatório destaca que A região Sudeste é responsável pela maior massa coletada dentre as demais regiões dopouco mais de 40 milhões de toneladas por ano, seguida das regiõespouco mais de 16,5 milhões de toneladas, ecerca de 8,5 milhões de toneladas coletadas.Mais lidosRestauração de recifes de corais e cantos da vida marinhaCidades flutuantes sustentáveis serão o nosso futuro?Helicoprion, um tubarão pré-histórico muito estranhoIlustração, Abrelpe.Importante ressaltar que, conforme já verificado anteriormente, enquanto as regiões Sudeste, Sul e Centro-Oeste já alcançaram índice de cobertura de coleta superior à média nacional, as regiões Norte e Nordeste ainda apresentam pouco mais de 80%, o que significa que em torno de 20% dos resíduos gerados não são alcançados pelos serviços de coleta regular nos municípios localizados nessas regiões.A coleta seletivaSobre a coleta seletiva houve um certo avanço. Segundo o relatório, Em 2020, o número de municípios que apresentaram alguma iniciativa de coleta seletiva foi de 4.145, representando 74,4% do total de municípios do país. Importante destacar, porém, que em muitos municípios as atividades de coleta seletiva ainda não abrangem a totalidade da população, podendo ser iniciativas pontuais. As regiões Sul e Sudeste são as que apresentam os maiores percentuais deiniciativa de coleta seletiva.Ilustração, Abrelpe.A omissão de30% dos prefeitosQuanto à destinação dos RSU, ou Resíduos Sólidos Urbanos, cerca de 30% dos municípios brasileiros ainda destinam os resíduos coletados sem nenhum tratamento prévio, o que contraria as normas vigentes e apresenta riscos diretos aos trabalhadores, à saúde pública e ao meio ambiente.Logística ReversaCom a vigência da Política Nacional de Resíduos Sólidos (Lei 12.3052010), a logística reversa foi o um dos instrumentos de implementação do princípio da partilhada pelo ciclo de vida dos produtos.Você sabe qual a média de lixo produzido por cada cidadão brasileiro? Aí está…Ilustração, Abrelpe.Diante disso, diversos setores passaram a ser responsáveis por encaminhar ações para a implementação de sistemas de logística reversa de produtos e embalagens pós-consumo, no intuito de priorizar seu retorno para um novo ciclo de aproveitamento.EMBALAGENS DE DEFENSIVOS AGRÍCOLASNo ano de 2020 demos esta boa notícia no postReciclagem de embalagens de agrotóxicos, boa notícia confirmada agora pelo relatório da Abrelpe O Sistema Campo Limpo, operado pelo Instituto Nacional de Processamento de Embalagens Vazias (inpEV), teve expressiva evolução em seu processamento passando de cerca de 31 mil toneladas em 2010 para quase 50 mil toneladas em 2020 (Gráfico 11), das quais 93,1% foram enviadas para reciclagem e 6,9% para incineração.Mas o relatório não fica apenas nas embalagens de defensivos agrícolas e aborda também pneus; lâmpadas; medicamentos; eletrônicos; baterias de chumbo-ácido irreversíveis; embalagens de aço; e embalagens em geral.Monitoramento prevenção bate ao lixo que vai para o marEste é outro dos tópicos do relatório da Abrelpe. Em resumo, eis o que diz Em todo o mundo, estima-se que a cada ano mais de 25 milhões de toneladas de resíduos sólidos têm os o destino. Cerca de 80% desse total são oriundos de atividades humanas desenvolvidas no continente…PUBLICIDADE Os Estados que fazem parte do programa Lixo Fora Dágua. Ilustração, Abrelpe.Deestimativas da ABRELPE, o paíscerca de 2 milhões de toneladas de resíduos sólidos que vazam para o mar anualmente, provenientes diretamente dos municípios litorâneos e regiões interiores, depois de percorrer grandes distâncias através de rios.Lixo Fora DÁguaEste foi um programa criado em 2018, fruto de um acordo de cooperação entre a ABRELPE e a Agência de Proteção Ambiental da Suécia (SEPA), tendo a cidade de Santos, no litoral de São o destino das ações pioneiras de monitoramento, prevenção bate ao lixo no mar e nos demais corpos hídricos (Este site já cansou de alertar que os rios do Brasil estão na UTI).Após a bem-sucedida experiência em Santos, o projeto teve seguimento e hoje11 municípios brasileiros, que representam 14 milhões de habitantes e integram o Programa Lixo Fora DÁgua promisso de aprimorar a gestão de resíduos o passo fundamental para a solução dessa problemática.E de onde vem o lixo que vai parar na costa brasileira?E de onde vem o lixo que vai parar na costa brasileira? Um estudo inédito realizado ema Universidade de Leeds, no Reino Unido, no âmbito do Programa Lixo Fora DÁgua, permitiu estimar que cerca de 690 mil toneladas de resíduos plásticos o destino os corpos hídricos, todos os anos no Brasil.Por corpos hídricos leia-se rios, a maioria dos quais está na UTI.Principais conclusões do estudoComo observado em vários momentos desta edição do Panorama dos Resíduos Sólidos no Brasil 2021, a pandemia da COVID19 trouxe novas dinâmicas para os serviços de limpeza urbana e manejo de resíduos sólidos nas cidades que além de registrar um aumento nas quantidades geradas e coletadas por conta da transferência dos centros de geração de resíduos para os domicílios, também ganharam mais visibilidade junto à população, principalmente pelo seu caráter de continuidade…Sistema linear de gestãoA partir dos dados apresentados, também se verifica que o país aindaum sistema linear de gestão de resíduos sólidos urbanos, apesar da vigência de uma Política Nacional de Resíduos Sólidos desde 2010…Prejudicando a saúde de 77,5 milhões de pessoasA despeito das diversas proibições existentes há décadas para impedir a poluição causada por resíduos sólidos, ainda vemos práticas de destinação inadequada presentes em todas as regiões dolixões a céu aberto ainda em pleno funcionamento, prejudicando a saúde de 77,5 milhões deum custo anual na casa dos bilhões de dólares para tratamento de saúde e mitigação da contaminação ambiental.PUBLICIDADE E, da mesma forma, a despeito das determinações para implantação obrigatória de sistemas de logística reversa, poucas iniciativas tornaram-se efetivas em âmbito nacional.Faltam recursosUma das conclusões do relatório é A falta de recursos aplicados no setor – R$ 0,36habdia para custeio de todos os serviços de limpeza urbana e manejo de resíduos sólidos em nossas cidades – influencia de maneira relevante os resultados observados (ou melhor, a falta de resultados).Falta prioridade para o temaA outra conclusão é a falta de prioridade A constatação de que, apesar de toda a legislação existente, o volume de RSU que segue para unidades de disposição inadequada continua aumentando, denota a falta de prioridade para o tema e a carência de recursos para financiar soluções que, além de dar cumprimento às determinações legais…Como mostra a Abrelpe, referência no assunto, o que de fato está faltando é ação do poder público. Está mais que na hora dos gestores priorizarem este que, além de um problema ambiental é, acima de tudo, um grave problema de saúde pública. Será que estes gestores já ouviram falar no conceito ESG, sustentabilidade, Agenda Verde, etc?A julgar pelo que nos traz o relatório, apenas uma minoria de nossos gestores já entraram no século 21. A grande maioria ainda está presa à questões menores, patrimonialistas, clientelistas, e corporativistas. É este o País do futuro?Leia aqui a íntegra do relatório.Imagem de abertura Edilson RodriguesAgência SenadoPolêmica da mineração submarina cresce no exteriorJoão Lara MesquitaCompartilhar ARTIGOS RELACIONADOSMais do autorPoluição brutal do Polo Petroquímico de Capuava é discutidaPolo Petroquímico Capuava, o novo Vale da Morte em SP?Barco de pesquisa e limpeza é o maior do mundo, conheçaComentários1 COMENTÁRIOMárcio Amazonas21 de dezembro de 2021 at 025João, bom reforço ao diagnóstico cada vez mais deprimente.Que bom que não elogiou a qualidade da legislação brasileira, que leva anos para ficar pronta, impraticável e não pegar. Lamentável endar que os gestores priorizem o saneamento profissionalizado.Eles sabem que o crime ambiental e contra a saúde pensa!Falta prioridade é para o ministério público e para a polícia.Obrigado por divulgar o trabalho da ABRELPE e priorizar o tema na sua coluna!ResponderDEIXE UMA RESPOSTA Cancelar respostaPlease enter ment!Please enter your name hereYou have entered an incorrect email address!Please enter your email address here ΔPopularesOs oceanos produzem oxigênio, você o?6 de agosto de 2021Lixo nos rios de Ubatuba; estância balneária até quando?15 de outubro de 2021Moradores de Ubatuba dizem não à verticalização24 de novembro de 2021Ilha Comprida está ameaçada pela especulação imobiliária4 de março de 2021E4fce3 de junho de 2021 AprendaAtive-sepodcastBiografiaAnuncieLojaProdutosImprensaContatoEmpregosNós nas redes Desenvolvido e Hospedado por InkID'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def limpa_texto(texto):\n",
    "    lista_simbolos = [':', '\\\\', '/', '\"', '\\'', '“', '”', '‘', '’']\n",
    "    texto = texto.replace('\\n', '').replace('\\u200b', '').replace('\\ufeff', '').replace('\\xa0', '')\n",
    "    texto = re.sub(r'http\\S+', '', texto)\n",
    "    texto = re.sub(r'Http\\S+', '', texto)\n",
    "    texto = re.sub(r'www\\S+', '', texto)\n",
    "    texto = re.sub(r'[\\S]+.com', '', texto)\n",
    "    texto = texto.replace('  ', '')\n",
    "    texto = texto.replace(' o o ', ' o ')\n",
    "    texto = texto.replace(' a a ', ' a ')\n",
    "    for simbolo in lista_simbolos:\n",
    "        texto = texto.replace(simbolo, \"\")\n",
    "    texto = texto.replace('  ', '').strip()\n",
    "    return texto\n",
    "\n",
    "conteudo = requests.get('https://marsemfim.com.br/os-lixoes-e-a-omissao-um-grave-problema-ambiental/').content\n",
    "soup = BeautifulSoup(conteudo)\n",
    "texto = limpa_texto(soup.get_text())\n",
    "texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'O Brasil é um país pra lá de curioso. Dá um passo pra frente, um pro lado, outro pra trás, para só depois prosseguir no avanço, sempre a passos de cágado. Aqui há outra curiosidade típica: leis que ‘pegam’, e leis que ‘não pegam’. Não conheço outro lugar onde este anacronismo ainda acontece. E assim vai se cristalizando o País do futuro. A gente nasce, cresce, envelhece, ouvindo a ladainha: o País do futuro. Sei… De vez em quando ainda aparece nova autoridade embalada em fracassos seguidos que insiste: ‘o Brasil vai surpreender o mundo’. Ora, já não surpreendemos o suficiente? O assunto de hoje são os lixões e a omissão, um grave problema ambiental.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(conteudo)\n",
    "info = soup.find(\"p\", {\"style\": \"text-align: justify;\"}).text\n",
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpa_texto(texto):\n",
    "    lista_simbolos = [':', '\\\\', '/', '\"', '\\'', '“', '”', '‘', '’']\n",
    "    texto = texto.replace('\\n', '').replace('\\u200b', '').replace('\\ufeff', '').replace('\\xa0', '')\n",
    "    texto = re.sub(r'http\\S+', '', texto)\n",
    "    texto = re.sub(r'Http\\S+', '', texto)\n",
    "    texto = re.sub(r'www\\S+', '', texto)\n",
    "    texto = re.sub(r'[\\S]+.com', '', texto)\n",
    "    texto = texto.replace('  ', '')\n",
    "    texto = texto.split('Comentários')[0]\n",
    "    texto = texto.split('COMENTÁRIOS')[0]\n",
    "    texto = texto.replace('imagem, reprodução twitter', '')\n",
    "    texto = texto.replace('Imagem, reprodução do Twitter', '')\n",
    "    texto = texto.replace('Imagem,', '')\n",
    "    texto = texto.replace('ARTIGOS RELACIONADOS', '')\n",
    "    texto = texto.replace('Mais do autor', '') \n",
    "    texto = texto.split('Imagem de abertura')[0]\n",
    "    for simbolo in lista_simbolos:\n",
    "        texto = texto.replace(simbolo, \"\")\n",
    "    inicio_texto = 'views'\n",
    "    fim_texto = 'Fontes'\n",
    "    texto = texto[texto.find(inicio_texto):].replace(inicio_texto, '')\n",
    "    texto = texto.replace('  ', '').strip()\n",
    "    return texto\n",
    "\n",
    "conteudo = requests.get('https://marsemfim.com.br/jose-bonifacio-de-andrada-e-silva-o-ecologista-do-imperio/').content\n",
    "soup = BeautifulSoup(conteudo)\n",
    "texto = limpa_texto(soup.get_text())\n",
    "texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showPaperSummary(texto):\n",
    "    maximo_palavras = 350\n",
    "    texto = ' '.join(texto.split(' ')[:maximo_palavras])\n",
    "    tldr_tag = \"\\n tl;dr:\"\n",
    "    openai.api_key = \"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\"\n",
    "    engine_list = openai.Engine.list() # calling the engines available from the openai api \n",
    "  \n",
    "    texto_gpt3 = texto + tldr_tag\n",
    "    response = openai.Completion.create(engine=\"davinci\",\n",
    "                                        prompt=texto_gpt3,\n",
    "                                        temperature=0.3,\n",
    "                                        max_tokens=500,\n",
    "                                        top_p=1,\n",
    "                                        frequency_penalty=0.3,\n",
    "                                        presence_penalty=0.0,\n",
    "                                        stop=[\"\\n\"]\n",
    "    )\n",
    "    resposta = response[\"choices\"][0][\"text\"].strip().lstrip(' ').capitalize()\n",
    "    resposta = '. '.join([frase.strip().capitalize() for frase in resposta.split('.')[:-1]]) + '.'\n",
    "    return resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showPaperSummary(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_article = BeautifulSoup(texto,'lxml')\n",
    "\n",
    "paragraphs = parsed_article.find_all('p')\n",
    "\n",
    "article_text = \"\"\n",
    "\n",
    "for p in paragraphs:\n",
    "    article_text += p.text\n",
    "# Removing Square Brackets and Extra Spaces\n",
    "article_text = re.sub(r'\\[[0-9]*\\]', ' ', article_text)\n",
    "article_text = re.sub(r'\\s+', ' ', article_text)\n",
    "# Removing special characters and digits\n",
    "formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )\n",
    "formatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)\n",
    "sentence_list = nltk.sent_tokenize(article_text)\n",
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "word_frequencies = {}\n",
    "for word in nltk.word_tokenize(formatted_article_text):\n",
    "    if word not in stopwords:\n",
    "        if word not in word_frequencies.keys():\n",
    "            word_frequencies[word] = 1\n",
    "        else:\n",
    "            word_frequencies[word] += 1\n",
    "    maximum_frequncy = max(word_frequencies.values())\n",
    "for word in word_frequencies.keys():\n",
    "    word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n",
    "    sentence_scores = {}\n",
    "    \n",
    "for sent in sentence_list:\n",
    "    for word in nltk.word_tokenize(sent.lower()):\n",
    "        if word in word_frequencies.keys():\n",
    "            if len(sent.split(' ')) < 30:\n",
    "                if sent not in sentence_scores.keys():\n",
    "                    sentence_scores[sent] = word_frequencies[word]\n",
    "                else:\n",
    "                    sentence_scores[sent] += word_frequencies[word]\n",
    "import heapq\n",
    "summary_sentences = heapq.nlargest(5, sentence_scores, key=sentence_scores.get)\n",
    "\n",
    "summary = ' '.join(summary_sentences)\n",
    "summary = '. '.join([frase.strip().capitalize() for frase in summary.split('.')[:-1]]) + '.'\n",
    "summary = summary.replace(' .', '.')\n",
    "summary = summary.replace('..', '.')\n",
    "summary = re.sub(\"(^|[.?!])\\s*([a-zA-Z])\", lambda p: p.group(0).upper(), summary)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json \n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "device = torch.device('cpu')\n",
    "\n",
    "text =\"\"\"\n",
    "The US has \"passed the peak\" on new coronavirus cases, President Donald Trump said and predicted that some states would reopen this month.\n",
    "The US has over 637,000 confirmed Covid-19 cases and over 30,826 deaths, the highest for any country in the world.\n",
    "At the daily White House coronavirus briefing on Wednesday, Trump said new guidelines to reopen the country would be announced on Thursday after he speaks to governors.\n",
    "\"We'll be the comeback kids, all of us,\" he said. \"We want to get our country back.\"\n",
    "The Trump administration has previously fixed May 1 as a possible date to reopen the world's largest economy, but the president said some states may be able to return to normalcy earlier than that.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "preprocess_text = text.strip().replace(\"\\n\",\"\")\n",
    "t5_prepared_Text = \"summarize: \"+preprocess_text\n",
    "print (\"original text preprocessed: \\n\", preprocess_text)\n",
    "\n",
    "tokenized_text = tokenizer.encode(t5_prepared_Text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "\n",
    "# summmarize \n",
    "summary_ids = model.generate(tokenized_text,\n",
    "                                    num_beams=4,\n",
    "                                    no_repeat_ngram_size=2,\n",
    "                                    min_length=30,\n",
    "                                    max_length=100,\n",
    "                                    early_stopping=True)\n",
    "\n",
    "output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print (\"\\n\\nSummarized text: \\n\",output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json \n",
    "from transformers import MT5Tokenizer, MT5ForConditionalGeneration, MT5Config\n",
    "\n",
    "model = MT5ForConditionalGeneration.from_pretrained('google/mt5-base')\n",
    "tokenizer = MT5Tokenizer.from_pretrained('google/mt5-base')\n",
    "device = torch.device('cpu')\n",
    "\n",
    "preprocess_text = texto\n",
    "prepared_text = \"summarize: \"+ preprocess_text\n",
    "print (\"original text preprocessed: \\n\", preprocess_text)\n",
    "\n",
    "tokenized_text = tokenizer.encode(prepared_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "\n",
    "# summmarize \n",
    "summary_ids = model.generate(tokenized_text,\n",
    "                             num_beams=4,\n",
    "                             no_repeat_ngram_size=2,\n",
    "                             min_length=30,\n",
    "                             max_length=100,\n",
    "                             early_stopping=True)\n",
    "\n",
    "output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print (\"\\n\\nResumo: \\n\",output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
